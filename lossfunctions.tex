\documentclass{article}

\begin{document}

\setlength{\parindent}{6ex}

\indent

Loss functions are used to measure the difference between ground-truth 
values and the predicted values in machine learning problems. Loss 
functions can also be called error functions. Thus, the aim is to minimize 
the value of loss function during training to have a better detector. 
In object detection problems, loss functions are composed of two parts: 
regression difference and classification difference. Loss functions are 
the measure of the difference between predicted regression box and 
ground-truth regression box and also, the difference between the correct 
class for object and the predicted class for the object. These functions can 
vary based on design choices and different loss functions have different 
advantages and disadvantages. The most frequent used loss functions as 
following:
\begin{itemize}
    \item L1 Loss Function
    
    L1 loss function minimizes the absolute differences between the
    predicted value and ground-truth value.

    \begin{equation}
        \sum_{i=1}^{n}\left(\left|y_{t}-y_{p}\right|\right)
    \end{equation}

    \item L2 Loss Function
    
    L2 loss function minimizes the squared differences between the 
    predicted value and ground-truth value.

    \begin{equation}
        \sum_{i=1}^{n}\left(y_{t}-y_{i}\right)^{2}
    \end{equation}

    \item Smooth L1 Loss Function
    
    Smooth L1 loss function minimizes whether half of the square of the 
    predicted value or difference of absolute value of predicted value and 
    0.5.

    \begin{equation}
        \sum_{i=1}^{n}\left\{\begin{array}{ll}
        {0.5 y_{i}^{2}} & {\text { if }|y_{i}|<1} \\
        {|y_{i}|-0.5} & {\text { otherwise }}
        \end{array}\right.
    \end{equation}

    \item Cross-Entropy Loss Function

    Cross-Entropy loss function minimizes the product of ground-truth value 
    and logarithm of predicted value.

    \begin{equation}
        \sum_{i=1}^{n}\left(y_{t} * \log \left(y_{i}\right)\right)
    \end{equation}

\end{itemize}    
\indent

Since the L1 loss function operates on the absolute values, it does not 
affect by outliers. Therefore, L1 is more robust to outliers. On the
other hand, L2 loss function operates on the squared values, therefore, 
L2 is not robust to outliers since outliers will cause a huge error value. 
However, if you analyze and see there are not many outliers in data, then, 
using the L2 loss function leads to better training. Smooth L1 loss function is a 
combination of L1 and L2 loss functions. When absolute value of loss is small, it 
behaves similar to L2 loss but when absolute value of loss is high, it behaves 
similar to L1 loss. Thus, it shares the advantages of both loss functions. Cross-entropy 
loss function measures the performance of classification. 
\end{document}